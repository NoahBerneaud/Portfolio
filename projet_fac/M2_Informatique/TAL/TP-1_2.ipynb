{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fWwijYPjIO3w"
   },
   "source": [
    "## TP 2: Transformers\n",
    "\n",
    "Pour ce TP, nous continuons à travailler sur une tâche de classification multi-classes. Cepandant, cette fois-ci, nous allons utiliser les modèles Transformers.\n",
    "\n",
    "<font color='red'>NB: Il est forcement recommandé de lancer ce notebook dans Colab ou sur votre machine à cause des ressources de GPU requierts pour entraîner les modèles aussi que les problèmes potentiels avec l'installation des bibliotèques. Si vous voulez utilisez votre propre machine pour ce TP et vous n'avez pas de carte vidéo, le modèle déjà entraîné est fourni plus loin dans le notebook.\n",
    "\n",
    "Pour ouvrir ce notebook dans Colab, suivez ce lien: https://colab.research.google.com/drive/1PgAtwKy43IWQVRqhPvZO4ZZHrefyvvAH?usp=sharing.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bDvm-j0XJPFb"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"tweet_eval\", 'emotion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q22FgJSsJ5D8"
   },
   "outputs": [],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "40ucYWXYJ9CT"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"distilbert-base-cased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QEGqLSfoKaKj"
   },
   "outputs": [],
   "source": [
    "tokenized_sample = tokenizer(dataset['train'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WitcuDBQLP1I"
   },
   "outputs": [],
   "source": [
    "tokenized_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l0w1_OOOKjn9"
   },
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokenized_sample['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djPN8V4PLWAp"
   },
   "source": [
    "### Question 1\n",
    "\n",
    "Donnez deux textes à la fois (c.-à-d. à deux paramètres pour `tokenizer`) au tokenizer et regardez la sortie. Qu'est-ce qu'il est différent de la sortie précédente ? Pensez à une tâche qui peut requérir ce genre de tokenisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D6JF2HvlLJUk"
   },
   "outputs": [],
   "source": [
    "tokenized_sample_2 = tokenizer(dataset['train'][1]['text'], dataset['train'][2]['text'])\n",
    "tokenizer.convert_ids_to_tokens(tokenized_sample_2['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u_nDOqJ3MWOZ"
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PdVpMDkUMyrJ"
   },
   "outputs": [],
   "source": [
    "print(tokenized_datasets['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iIFcIvbOM4ei"
   },
   "outputs": [],
   "source": [
    "num_labels = dataset['train'].info.features['label'].num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtB1hOfBazni"
   },
   "source": [
    "Initialisons le modèle pre-entraîné."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ooqytAdXNS3v"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hLXURxPZOFdZ"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-tweeteval\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    optim='adamw_torch',\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HbQR41ydRVaW"
   },
   "source": [
    "### Question 2\n",
    "\n",
    "Lisez la documentation de [TrainingArguments](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/trainer#transformers.TrainingArguments). Décrivez tous les paramètres d'entraînement ci-dessus. Selon la documentation, quels sont les autres paramètres qui peuvent influencer l'entraînement de modèle le plus ?\n",
    "\n",
    "Après, n'hésitez pas à changer ou ajouter les paramètres selon vos préferences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xjvZVpUGSRHv"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUR-8dI4SnPc"
   },
   "source": [
    "Pour calculer les métriques, nous utilisons la bibliotèque [evaluate](https://huggingface.co/docs/evaluate/index). Pour l'accuracy, sklearn est utilisé \"sous le capot\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g178mJRsONuS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s5l9KHbGOSpz"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ORChJk1Tf_2"
   },
   "source": [
    "_NB: Si vous n'avez pas de ressources pour l'entraînement ou vous ne voulez pas attendre, vous pouvez sauter cette étape de l'entraînement et télécharger le modèle déjà entraîner dans le code qui suit._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F855nNUgOWVV"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AbFqyI7LUwLz"
   },
   "source": [
    "Une bonne chose à faire, c'est de créer aussi la carte de modèle et sauvgarder le tokenizer. Comme ça, vous pouver partager très facilement votre modèle et l'utiliser après.\n",
    "\n",
    "Regardez les autres paramètres de [`create_model_card`](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/trainer#transformers.Trainer.create_model_card) et n'hésitez pas à les changer selon vos préferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gg6QwyZrOpbc"
   },
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "\n",
    "trainer.save_model()\n",
    "trainer.create_model_card(language='en', dataset='tweet_eval')\n",
    "tokenizer.save_pretrained(f\"{model_name}-finetuned-tweeteval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J2iBaQQIVlwT"
   },
   "source": [
    "Décommentez la ligne ci-dessous pour télécharger le modèle déjà entraîné sur le même jeu de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j1YTmZaYLmiz"
   },
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"501Good/distilbert-base-cased-finetuned-tweeteval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1i9v2b2Wekz"
   },
   "source": [
    "## Question 3\n",
    "\n",
    "Comme dans le TP1, on peut aussi visualiser l'attention.\n",
    "\n",
    "Faites la prédiction du premier texte de \"test set\" avec le modèle entraîné en sauvgardant toujours les attentions de modèle (consultez la documentation de [`DistilBertForSequenceClassification`](https://huggingface.co/docs/transformers/v4.28.1/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification)). Après, visualisez l'attention avec `head_view` et `model_view` dans le Bertviz. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aclH1xtpVBLR"
   },
   "outputs": [],
   "source": [
    "from bertviz import head_view, model_view\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = dataset['test'][0]['text']\n",
    "label = dataset['test'][0]['label']\n",
    "\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "outputs = model(**inputs, output_attentions=True)\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "head_view(outputs.attentions, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"La phrase est : \", text)\n",
    "print(\"L'emotion de la phrase est : \", label)\n",
    "print(\"La prédiction du modèle est : \", model(**inputs).logits.argmax().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = 0\n",
    "for i in range(100):\n",
    "    text = dataset['test'][i]['text']\n",
    "    label = dataset['test'][i]['label']\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    outputs = model(**inputs, output_attentions=True)\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "\n",
    "    if label == model(**inputs).logits.argmax().item():\n",
    "        acc += 1\n",
    "print(\"L'accuracy est de : \", acc/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_view(outputs.attentions, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ao5T2loGYwy7"
   },
   "source": [
    "## Exercice\n",
    "\n",
    "L'intégration de connaissances externes dans les modèles est un moyen possible d'améliorer les performances des modèles. Une possibilité consiste à marquer le texte annoté dans le texte d'entrée, ce qui permet au modèle d'accorder plus d'attention à ces passages annotés.\n",
    "\n",
    "Utiliser le lexique des émotions fourni et marquer les mots correspondants dans le texte d'entrée. Faire des prédictions basées sur le texte modifié et analyser les résultats.\n",
    "\n",
    "A la fin du cours (pendant le dernier TP), vous devez présenter vos résultats et votre analyse pour les exercices (TP2 et TP4). Il s'agira d'une courte présentation de 5 à 10 minutes et vous devrez soumettre les diapositives correspondantes pour évaluation avec le code. Les deux exercices seront notés sur 10 points et le score total sera ajouté à la note finale du cours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "lexique_emotion = pd.read_csv('NRC-Emotion-Lexicon-Wordlevel-v0.92.txt', sep='\\t', header=None, names=['word', 'emotion', 'value'])\n",
    "lexique_emotion.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ge9abDJcY24_"
   },
   "outputs": [],
   "source": [
    "def annotate_text(texts,lexicon):\n",
    "    dataset = []\n",
    "    for text in texts:\n",
    "        words = text.split()\n",
    "        annotate_text = []\n",
    "        \n",
    "        for word in words:\n",
    "            if word.lower() in lexicon.word.values:\n",
    "                annotate_text.append(\"[SENT] \" + word + \" [SENT]\")\n",
    "            else:\n",
    "                annotate_text.append(word)\n",
    "        new_text = ' '.join(annotate_text)\n",
    "        dataset.append(new_text)\n",
    "    return dataset\n",
    "\n",
    "new_dataset_train = dataset['train'].map(lambda x: {'text': annotate_text(x['text'], lexique_emotion)}, batched=True)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset_eval = dataset['validation'].map(lambda x: {'text': annotate_text(x['text'], lexique_emotion)}, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokenized_datasets_train = new_dataset_train.map(tokenize_function, batched=True)\n",
    "new_tokenized_datasets_eval = new_dataset_eval.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokenized_datasets_train.set_format(type='torch', device='cuda')\n",
    "new_tokenized_datasets_eval.set_format(type='torch', device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-tweeteval\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    optim='adamw_torch',\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer \n",
    "\n",
    "if(torch.cuda.is_available()):\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"There are %d GPU(s) available.\" % torch.cuda.device_count())\n",
    "else : \n",
    "    print(\"No GPU available, using the CPU instead.\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=new_tokenized_datasets_train,\n",
    "    eval_dataset=new_tokenized_datasets_eval,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model()\n",
    "trainer.create_model_card(language='en', dataset='tweet_eval')\n",
    "tokenizer.save_pretrained(f\"{model_name}-finetuned-tweeteval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = 0\n",
    "for i in range(100):\n",
    "    text = annotate_text(dataset['test'][i]['text'], lexique_emotion)\n",
    "    label = dataset['test'][i]['label']\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    outputs = model(**inputs, output_attentions=True)\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "\n",
    "    if label == model(**inputs).logits.argmax().item():\n",
    "        acc += 1\n",
    "print(\"L'accuracy est de : \", acc/100)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
